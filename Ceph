DAS存储(直连存储,接口有限)
NAS存储(网络附加存储,提供文件系统共享nfs,samba)
SAN存储(存储区域网络iscsi)
SDS分布式存储(软件定义存储)
常用分布式文件系统Lustre,Hadoop,FastDFS,Ceph,GlusterFS

Ceph
分布式文件系统,具有高扩展,高可用,高性能的特点
要求集群之间时间误差0.05ms


部署ceph集群
node1主机
事先准备好对其他机的免钥连接,/etc/hosts域名解析能识别其他主机,/etc/chrony时间同步以node1为主,yum正常使用
yum -y install ceph-deploy            安装软件
mkdir ceph-cluster                    新建一个自定义名字的目录(ceph相关操作全在里面)
cd ceph-cluster/                      进入该目录
ceph-deploy new node1 node2 node3     新建集群主机的配置文件(需要能解析其他主机的IP)
  for i in node1 node2 node3          给所有节点安装ceph相关软件包
do
    ssh  $i "yum -y install ceph-mon ceph-osd ceph-mds ceph-radosgw"
done 
ceph-deploy mon create-initial       初始化所有节点的mon服务，也就是启动mon服务（主机名解析必须对）
ceph -s           查看状态(HEALTH_ERR为错误)
创建OSD(缓存盘)
     for i in node1 node2 node3                vdb1和vdb2这两个分区用来做存储服务器的journal缓存盘
do
     ssh $i "parted /dev/vdb mklabel gpt"
     ssh $i "parted /dev/vdb mkpart primary 1 50%"
     ssh $i "parted /dev/vdb mkpart primary 50% 100%"
 done
    for i in node1 node2 node3             磁盘分区后的默认权限无法让ceph软件对其进行读写操作，需要修改权限。node1、node2、node3都需要操作，这里以node1为例。         
> do
> ssh $i "chown ceph:ceph /dev/vdb1"        此配置为临时权限,重启不生效
> ssh $i "chown ceph:ceph /dev/vdb2"
> done
vim /etc/udev/rules.d/70-vdb.rules         永久配置设备权限(此文件为新建文件)
ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"
初始化清空磁盘数据（仅node1操作即可）
 ceph-deploy disk  zap  node1:vdc   node1:vdd    
 ceph-deploy disk  zap  node2:vdc   node2:vdd
 ceph-deploy disk  zap  node3:vdc   node3:vdd  
创建OSD存储空间（仅node1操作即可）
ceph-deploy osd create  node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2  vdc为储存盘,vdb1为缓存盘
ceph-deploy osd create  node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
ceph-deploy osd create  node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2
ceph -s           查看状态(HEALTH_OK为正确)


创建Ceph块存储
在node1上操作
ceph osd lspools                    查看存储池
rbd create demo-image --image-feature  layering --size 10G    创建镜像  #这里的demo-image和image(默认前面有rbd,不打也可以)为创建的镜像名称，可以为任意字符。
rbd create rbd/image --image-feature  layering --size 10G     创建镜像   #--image-feature参数指定我们创建的镜像有哪些功能，layering是开启COW功能
rbd list                        查看现有镜像名字
rbd info 镜像名                  查看镜像详细信息
rbd resize --size 7G 镜像名 --allow-shrink     缩小容量(慎重操作)allow-shrink为允许缩小
rbd resize --size 15G 镜像名                   增加容量
scp ceph.client.admin.keyring   client:/etc/ecph  发送秘钥给客户端
客户端通过KRBD访问
yum -y  install ceph-common               安装软件
 scp 192.168.4.11:/etc/ceph/ceph.conf  /etc/ceph/         接收秘钥
 scp 192.168.4.11:/etc/ceph/ceph.client.admin.keyring  /etc/ceph/     接收 秘钥
 rbd map 镜像名         获取镜像盘`
 lsblk                 查看磁盘分区
 rbd showmapped                           查看镜像





ceph-deploy mon create-initial常见错误及解决方法（非必要操作，有错误可以参考）：
如果提示如下错误信息：
[node1][ERROR ] admin_socket: exception getting command descriptions: [Error 2] No such file or directory
解决方案如下（在node1操作）：
先检查自己的命令是否是在ceph-cluster目录下执行的！！！！如果确认是在该目录下执行的create-initial命令，依然报错，可以使用如下方式修复。
[root@node1 ceph-cluster]# vim ceph.conf      #文件最后追加以下内容
public_network = 192.168.4.0/24
修改后重新推送配置文件:
[root@node1 ceph-cluster]# ceph-deploy --overwrite-conf config push node1 node2 node3


如果查看状态包含如下信息：
health: HEALTH_WARN
        clock skew detected on  node2, node3…  
clock skew表示时间不同步，解决办法：请先将所有主机的时间都使用NTP时间同步！！！
Ceph要求所有主机时差不能超过0.05s，否则就会提示WARN，如果使用NTP还不能精确同步时间，可以手动修改所有主机的ceph.conf，在[MON]下面添加如下一行：
mon clock drift allowed = 1
如果状态还是失败，可以尝试执行如下命令，重启ceph服务：
[root@node1 ~]#  systemctl restart ceph\*.service ceph\*.target

