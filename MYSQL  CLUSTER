主从服务器结构
一主多从结构:从--->主<----从
主从从结构:主<--从<--从      中间机子配置文件加入log_slave_updates   允许级联复制
主主结构:主<----->主

mysql主从同步,主更新会在从上更新,从更新与主无关
原理master启用binlog日志;
slave_IO复制master的binlog日志到本机的relay-log文件,slave_SQL执行relay-log里面的语句确保和主机一样
/var/lib/mysql/master.info记录主机配置信息  /var/lib/mysql/relay-log.info记录本机配置信息
cd /var/lib/mysql/              进入配置文件目录
rm -rf master.info  relay-log.info  *relay-bin*   清除主从配置文件
主服务器配置:
vim /etc/my.cnf                      修改配置文件
log-bin=日志文件名                     定义binlog日志文件
server_id=服务器id号                   定义服务器id号
grant replication slave on *.* to slave@'%' identified by '123456';     授权从属用户
show master status;                  查看日志状态
mysqldump -uroot -p123456 --master-data 库 > /root/xx.sql     导出数据库文件   master-data记录日志名称和偏移量
scp /root/xx.sql root@192.168.4.52:/                          发送数据库文件给从服务器
从服务器配置:
vim /etc/my.cnf                       修改配置
server_id=服务器id号                    定义服务器id号,与主服务器不同
mysql -uroot -p123456  库  <  /xx.sql   导入数据库文件
vim /xx.sql                         查看日志文件名和偏移量
change master to master_host='192.168.4.51',        输入主库信息
    -> master_user='slave',
    -> master_passowrd='123456',
    -> master_log_file='日志名',
    -> master_log_pos=偏移量;
start slave;                          开启进程
show slave status;                    查看状态(里面有排错信息)

复制模式介绍
异步复制(默认 ):主库执行完一次事务后,立即返回给客户端,不管从库是否接受并处理
全同步复制:主库执行玩一次事务,并且所有从库执行了该事务,才返回结果给客户端
半同步复制:主库执行完一次事务后,等待至少一个从库收到并写到relay log中才返回给客户端
show  variables like 'have_dynamic_loading';        查看是否允许加载动态模块(默认允许)
install plugin  rpl_semi_sync_master  SONAME  ”semisync_master.so”;    加载master模块,在主库上执行
install plugin  rpl_semi_sync_master  SONAME  ”semisync_slave.so”;    加载slave模块,在从库上执行
select plugin_name, plugin_status from information_schema.plugins where  plugin_name like '%semi%';  查看模块是否安装成功
set  global rpl_semi_sync_master_enabled = 1;   启用 master 半同步复制,在主库执行,默认关闭
set  global rpl_semi_sync_slave_enabled = 1;    启用 slave 半同步复制,在从库执行,默认关闭
show variables like "rpl_semi_sync_%_enabled";  查看半同步模式模块是否启用
修改配置文件,永久启用半同步复制
vim /etc/my.cnf
plugin-load="rpl_semi_sync_master=semisync_master.so"      加载master模块,在主库上配置
plugin-load="rpl_semi_sync_slave=semisync_slave.so"        加载slave模块,在从库上配置
rpl-semi-sync-master-enabled = 1           启用 master 半同步复制,在主库配置
rpl-semi-sync-slave-enabled = 1            启用 slave 半同步复制,在从库配置


mysql读写分离软件(中间件),在主库写入,在从库读取
mycat,mysqlproxy,maxscale都免费
部署maxscale服务
安装maxscale.rpm包
vim /etc/maxscale.cnf       修改配置文件
threads=auto                定义线程个数
[server1]                   后端服务主机(复制这一段可新建一个后端主机)
type=server
address=192.168.4.51        后端主机ip地址
port=3306                   端口
protocol=MySQLBackend       
[MySQL Monitor]             监视段
type=monitor
module=mysqlmon
servers=server1,server2     要监视的主机
user=maxscalemon            监视用户
passwd=123456               密码
monitor_interval=10000
#[Read-Only Service]        注释这一段,不定义只读服务
[Read-Write Service]
type=service
router=readwritesplit       调用模块
servers=server1,server2     
user=maxscaleroute          路由用户(客户端访问调度器时,调度器用于访问后端服务器的用户)
passwd=123456
max_slave_connections=100%
#[Read-Only Listener]       注释这一段,不监听只读服务
[Read-Write Listener]       这段监听读写可以修改端口
[MaxAdmin Listener]         这段监听管理可以定义端口
port=4007
maxscale -f /etc/maxscale.cnf    启动程序(先配置后端主服务器)
killall  -9  maxscale            停止进程
maxadmin -uadmin -pmariadb -P4007   连接到监控信息,-P管理段的端口
list servers                        查看后台服务器状态
配置后端主服务器
grant replication slave,replication client on *.* to maxscalemon@'%' identified by '123456';   授权监视用户
grant select on mysql.* to maxscaleroute@'%' identified by '123456';            授权路由用户
客户端测试连接
mysql -h调度器ip -P监听读写端口 -u主库的授权用户 -p密码   ip为调度器ip,端口为监听读写的端口,用户是主库上的用户


多实例数据库
一台物理主机上允许多个数据库服务,节约运维成本,提高硬件利用率
解压mysql-5.7.20-linux-glibc包,yum安装libaio依赖包
mv mysql-5.7.20-linux-glibc2.12-x86_64/ /usr/local/mysql  把解压出的目录移动到软件目录下
重新配置/etc/my.cnf
[mysqld_multi]                                           启动多实例服务
mysqld=/usr/local/mysql/bin/mysqld_safe                   指定进程文件路径
mysqldamin=/usr/local/mysql/bin/mysqladmin               指定管理命令路径
user=root                                                指定进程用户
[mysqld1]                     定义数据库1(可定义多个)
datadir=/dir1                 数据库目录需要手动创建
port=3307                     数据库端口
log-error=/dir1/mysql1.log    错误日志位置
pid-file=/dir1/mysql1.pid     进程pid号文件位置
socket=/dir1/mysql1.sock      指定sock文件的路径和名称,用于传输数据
[mysqld2]
datadir=/dir2
port=3308
log-error=/dir2/mysql2.log
pid-file=/dir2/mysql2.pid
socket=/dir2/mysql2.sock
PATH=/usr/local/mysql/bin:$PATH    把命令路径加到path变量(临时)
echo 'export  PATH=/usr/local/mysql/bin:$PATH' >> /etc/profile   把命令路径加到path变量(永久)
useradd mysql           新建mysql用户
/usr/local/mysql/bin/mysqld_multi  start 1           启动mysqld1数据库,1是实例编号
/usr/local/mysql/bin/mysqld_multi  --user=root  --password=密码 stop 实例编号  停止服务
/usr/local/mysql/bin/mysql  -uroot -p初始密码  -S sock文件绝对路径   本机连接数据库
mysql -hip地址 -P端口 -u用户 -p密码           客户端访问实例数据库


数据分片
将存放在一台数据库服务器中的数据,按照特定的方式进行拆分,分散在多台数据库服务器中
mycat是基于java分布式数据库系统中间件,为高并发环境的分布式存储提高解决方案,适合数据大量写入的存储需求,提供数据读写分离服务,数据分片服务
工作过程:解析命令涉及到的表,看表配置,如果有分片规则,调用分片函数,发送命令到对应的分片服务器去执行
主要文件 server.xml定义连接到分片机的用户  rule.xml定义分片规则函数  schema.xml定义逻辑库,表,连接用户,后台主机,表结构
分片规则:枚举法sharding-by-intfile   固定分片rule1  范围约定anto-sharding-long  求模法mod-long  日期列分区法sharding-by-date  一致性hash sharding-by-murmur
sharding-by-pattern通配取模  ASCII码求模通配sharding-by-prefixpattern  编程指定sharding-by-substring  字符串拆分hash解析sharding-by-stringhash
部署mycat
yum安装java-1.8.0-openjdk,解压Mycat-server-1.6-RELEASE,移动到/usr/local下
server.xml设置连接账号及逻辑库名   schema.xml配置数据分片  rule.xml分片规则  *.txt函数调用文件  wrapper.log服务启动日志  mycat.log记录报错内容
sed -i '56,77d' /usr/local/mycat/conf/schema.xml    删除备注行 (删之前先备份主配置文件)
sed -i '16,18d' /usr/local/mycat/conf/schema.xml 
sed -i '36,39d' /usr/local/mycat/conf/schema.xml
vim /usr/local/mycat/conf/schema.xml   定义后台主机,表规则,连接用户
<schema>...</schema>定义分片信息   name逻辑库   dataNode指定数据节点名   rule指定使用的分片规则   type=global数据不分片存储,每台主机都可查
<dataNode/>定义数据节点   name数据节点  datahost数据库服务器主机名  database数据库名(需在相应服务器上创建同名的)
<datahost name=hostname1><writeHost host="hostM1" url="192.168.4.53:3306" user="big"password="123456"></datahost> 定义主机名的ip地址
/usr/local/mycat/bin/mycat/ start    启动服务(端口8066),可--help查看命令
mysql -h分片主机IP -P8066 -u用户 -p密码  客户端连接分片服务器,里面有个TESTDB库,库里面的表是配置文件里定义的表,需要重新定义表结构
sharding-by-intfile枚举法分片规则配置
新建employee表中有sharding_id字段int类型,会根据sharding_id的值把数据分配到不同数据库主机里
vim /usr/local/mycat/conf/partition-hash-int.txt
10000=0  数据存储在dn1库里
10010=1  数据存储在dn2库里
10020=2  数据存储在dn3库里
重启服务  
mod-long求模法分片规则配置
新建hotnews表根据id字段对3取余,得到0,1,2分别存入三台数据库主机
vim /usr/local/mycat/conf/rule.xml 文件可以修改取余数
重启服务
添加逻辑库,表
vim /usr/local/mycat/conf/schema.xml   修改配置文件
<schema>...</schema>                   新建schema段,修改里面的配置
vim /usr/local/mycat/conf/server.xml   修改配置文件
<user name="root">
<property name="schemas">TESTDB,新库名</property>  添加新库名
重启服务


MHA高可用数据库切换
数据库的自动故障切换能在0~30秒之内完成
环境部署
yum -y install perl-*      在集群相关的所有主机上安装系统自带的perl软件包(还有另外的perl包上网找资源)
yum -y install perl-*      安装mha包里的perl包
ssh-keygen -f /root/.ssh/id_rsa -N ''   创建秘钥对,实现无密码连接
ssh-copy-id root@目标ip     服务器之间可以无密码连接,管理器和服务器之间也可以
管理器
rpm -ivh mha4mysql-node-0.56-0.el6.noarch.rpm  安装提供MHA程序的软件包
tar -zxf mha4mysql-manager-0.56.tar.gz
cd mha4mysql-manager-0.56/
perl Makefile.PL
make && make install
mkdir /etc/mha         新建配置文件夹
vim  /etc/mha/app1.cnf      手写配置文件
[server default]            管理服务默认配置
manager_workdir=/etc/mha    工作目录
manager_log=/etc/mha/manager.log    日志文件
master_ip_failover_script=/etc/mha/master_ip_failover   故障切换脚本
ssh_user=root       访问ssh服务用户
ssh_port=22         ssh服务端口
repl_user=repluser  主从连接数据授权用户
repl_password=123qqq...A    密码
user=root           管理器连接数据库的监控用户
password=123qqq...A         密码
[server1]               指定第一台数据库服务器
hostname=192.168.4.51   服务器IP地址
candidate_master=1      竞选主库
[server2]
hostname=192.168.4.52
candidate_master=1
[server3]
hostname=192.168.4.53
candidate_master=1
cp master_ip_failover /etc/mha     复制切换脚本到配置文件夹下
chmod +x /etc/mha/master_ip_failover       给执行权限
vim +35 /etc/mha/master_ip_failover         修改切换脚本第35行
$vip = '192.168.4.100/24';  # Virtual IP    定义vip地址
数据库节点配置(先配置成一主多从结构)
ifconfig eth0:1 192.168.4.100     主数据库服务器配置VIP(仅在主库机上配置)
grant replication slave on *.* to repluser@'%' identified by '123qqq...A';  授权主从连接用户(与管理器上配置文件中的一致)
grant all on *.* to root@'%' identified by '123qqq...A';  监控用户(与管理器上配置文件中的一致)
vim /etc/my.cnf      修改配置文件,每台节点都要
server_id=53             定义服务器id
log-bin=db53             开启binlog日志
plugin-load="rpl_semi_sync_slave=semisync_slave.so;rpl_semi_sync_master=semisync_master.so"    加载主从模块
rpl-semi-sync-master-enabled = 1    启用master模块
rpl-semi-sync-slave-enabled = 1     启用slave模块
relay_log_purge=0         关闭自动删除中继日志文件
rpm -ivh mha4mysql-node-0.56-0.el6.noarch.rpm   安装mha_node包,每台节点都要
管理器测试
masterha_check_ssh --conf=/etc/mha/app1.cnf    测试ssh是否通畅,在管理器上操作
masterha_check_repl --conf=/etc/mha/app1.cnf   测试主从同步配置,在管理器上操作
管理器开启服务
masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover  启动管理服务,删除宕机主库的配置,开启8小时内无限切换(不开启则8小时内只能切换一次)
masterha_check_status --conf=/etc/mha/app1.cnf   开启服务为前台服务,此命令可检测是否正在运行
masterha_stop    --conf=/etc/mha/app1.cnf   停止服务
现在,客户端可以访问管理器的VIP来访问数据库,数据会在三台后台数据库中备份,一旦发生故障,管理器会切换主库
修复故障服务器
数据库集群中主库停止服务时,管理器的配置文件会删除那台机的记录,需要手动逻辑备份当前的主库,再修复配置文件,在重启管理器服务
mysqldump -uroot -p123456 --master-data db9 > db9.sql    先现任主库机上做完全备份
scp db9.sql root@192.168.4.51:/root/       发送给故障机
mysql -uroot -p123456 db9 < /root/db9.sql   故障机导入完全备
vim /root/db9.sql        查看主库的日志名和偏移量
change  master  to ... ; 在故障机上重设主库信息
start slave;             故障机开启slave进程
vim /etc/mha/app1.cnf    在管理机上重新添加故障机的信息
masterha_check_ssh --conf=/etc/mha/app1.cnf    管理机测试SSH
masterha_check_repl --conf=/etc/mha/app1.cnf   管理机测试主从同步配置
masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover 启动管理服务

PXC高可用数据库集群
无同步延迟,没有主从切换操作,无需VIP,支持innodb引擎,多线程复制,节点自动加入,节点修复后自动同步数据
涉及软件:percona-xtrabackup在线热备程序   qpress递归压缩程序   Percona-XtraDB-Cluster-server集群服务程序
3306数据库服务端口  4444 SST全量同步端口  4567集群通信端口   4568IST增量同步端口
PXC环境搭建
所有节点在/etc/hosts上必须能映射其他节点的主机名,PXC环境自带mysql
yum -y install libev*.rpm  percona-xtrabackup*.rpm  qpress*.rpm  Percona-XtraDB-Cluster*.rpm  安装软件和依赖包
cd   /etc/percona-xtradb-cluster.conf.d      进入配置文件目录
mysqld.cnf  数据库配置文件     mysqld_safe.cnf  percona server配置文件   wsrep.cnf PXC集群配置文件
vim    mysqld.cnf       修改数据库配置文件
server-id=*             每台节点修改服务器id
vim    wsrep.cnf        修改集群配置文件
wsrep_cluster_address=gcomm://192.168.4.71,192.168.4.72,192.168.4.73   集群成员列表
wsrep_node_address=192.168.4.71      本机IP地址
wsrep_cluster_name=pxc-cluster       集群名称,3台必须相同
wsrep_node_name=pxcnode1             本机主机名(与主机解析相同)
wsrep_sst_auth="sstuser:123qqq...A"  SST数据同步授权用户
启动服务
systemctl start mysql@bootstrap.service  启动集群服务(在一台节点上做)
grep pass  /var/log/mysqld.log             查看初始密码
alter user root@'localhost' identified by '123456'  修改登录密码
grant reload,lock tables,replication client,process on *.* to sstuser@'localhost' identified by '123qqq...A';  添加授权用户,与集群配置文件的一致
现在在其他节点上启动服务systemctl start mysql(如果其他节点启动失败,检查集群配置文件,删除在节点上/var/lib/mysql下的文件再启动)
killall -9 mysqld      杀死进程
ps -C mysqld           查看进程号
mysql -uroot -p123456                      其他节点可以用同一个root用户登录
show status like '%wsrep%';                 查看集群信息(任意节点)
wsrep_incoming_addresses   集群成员列表
wsrep_cluster_size     集群服务器台数(主键自增数)
wsrep_cluster_status   集群状态
wsrep_connected        连接状态
wsrep_ready            服务器状态























